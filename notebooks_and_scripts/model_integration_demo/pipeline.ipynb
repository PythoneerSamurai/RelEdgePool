{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **This is an example notebook that shows how to integrate RelEdgePool (it's components) in a 3D deep learning pipeline.**\n",
    "##### RelEdgePool is integrated in your model, so the rest of your pipeline remains the same.\n",
    "---"
   ],
   "id": "d41ff28eeb63ce2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Imports\n",
    "---"
   ],
   "id": "74d7453f85f43889"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch3d.io import load_objs_as_meshes\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "'''\n",
    "    The GraphConvolver class has logic for performing graph convolution on batched meshes having different numbers of vertices and edges.\n",
    "    Similarly, the PoolingExecutor class has logic for performing pooling with RelEdgePool on batched meshes having different number of vertices and edges\n",
    "'''\n",
    "from utils.convolution_executor import GraphConvolver\n",
    "from utils.pooling_executor import PoolingExecutor\n",
    "from utils.experimentation_datasets_dataloader import ExperimentationDatasets"
   ],
   "id": "1aae79b8d26a8557",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data loading\n",
    "---"
   ],
   "id": "59c0380dd4e0bde8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"",
   "id": "f8437bdb685e3144",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def collate_meshes(batch):\n",
    "    \"\"\"\n",
    "    This function takes the mesh paths generated by the ExperimentalDatasets class and batched by the Dataloader.\n",
    "    It loads meshes from the batched mesh paths, and batches their vertices and edges.\n",
    "    \"\"\"\n",
    "    mesh_paths, labels = zip(*batch)\n",
    "\n",
    "    # Load meshes from paths\n",
    "    meshes = load_objs_as_meshes(list(mesh_paths), load_textures=False, device=device)\n",
    "\n",
    "    # Get padded vertices and individual edges\n",
    "    vertices_batch = meshes.verts_padded()\n",
    "    edges_batch = [mesh.edges_packed() for mesh in meshes]\n",
    "\n",
    "    # Convert labels to tensor\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "\n",
    "    return vertices_batch, edges_batch, labels_tensor"
   ],
   "id": "63223876b93740c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset_root_directory = \"../../datasets/shrec\" # choose shrec or cubes\n",
    "try:\n",
    "    train_dataset = ExperimentationDatasets(dataset_root_directory, split='train')\n",
    "    test_dataset = ExperimentationDatasets(dataset_root_directory, split='test')\n",
    "except FileNotFoundError:\n",
    "    print(\"Dataset not found. Make sure to execute the relevant script in the datasets folder.\")"
   ],
   "id": "14ec2dcf1c487337",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_meshes)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_meshes)"
   ],
   "id": "2ae014dae484f6a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Model Architecture\n",
    "---"
   ],
   "id": "d66319ef3f9db73f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "    A dummy graph convolutional neural network that has RelEdgePool integrated in it.\n",
    "\"\"\"\n",
    "class DummyGCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Graph convolution layers\n",
    "        self.gc1 = GraphConvolver(in_channels=3, out_channels=16)\n",
    "        self.gc2 = GraphConvolver(in_channels=16, out_channels=32)\n",
    "        self.gc3 = GraphConvolver(in_channels=32, out_channels=64)\n",
    "        self.gc4 = GraphConvolver(in_channels=64, out_channels=32)\n",
    "        self.gc5 = GraphConvolver(in_channels=32, out_channels=16)\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(normalized_shape=16)\n",
    "        self.ln2 = nn.LayerNorm(normalized_shape=32)\n",
    "        self.ln3 = nn.LayerNorm(normalized_shape=64)\n",
    "        self.ln4 = nn.LayerNorm(normalized_shape=32)\n",
    "        self.ln5 = nn.LayerNorm(normalized_shape=16)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.l1 = nn.Linear(in_features=16, out_features=32)\n",
    "        self.l2 = nn.Linear(in_features=32, out_features=30)  # Adjust out_features based on the number of classes in your dataset\n",
    "\n",
    "    def forward(self, vertices_batch, edges_batch):\n",
    "        # First conv block\n",
    "        vertices_batch = self.gc1.convolve(vertices_batch, edges_batch)\n",
    "        vertices_batch = self.ln1(vertices_batch)\n",
    "        vertices_batch = nn.functional.leaky_relu(vertices_batch, 0.2)\n",
    "        vertices_batch = self.dropout(vertices_batch)\n",
    "        vertices_batch, edges_batch = PoolingExecutor(vertices_batch, edges_batch).pool()  # first pooling\n",
    "\n",
    "        # Second conv block\n",
    "        vertices_batch = self.gc2.convolve(vertices_batch, edges_batch)\n",
    "        vertices_batch = self.ln2(vertices_batch)\n",
    "        vertices_batch = nn.functional.leaky_relu(vertices_batch, 0.2)\n",
    "        vertices_batch = self.dropout(vertices_batch)\n",
    "        vertices_batch, edges_batch = PoolingExecutor(vertices_batch, edges_batch).pool()  # second pooling\n",
    "\n",
    "        # Third conv block\n",
    "        vertices_batch = self.gc3.convolve(vertices_batch, edges_batch)\n",
    "        vertices_batch = self.ln3(vertices_batch)\n",
    "        vertices_batch = nn.functional.leaky_relu(vertices_batch, 0.2)\n",
    "        vertices_batch = self.dropout(vertices_batch)\n",
    "        vertices_batch, edges_batch = PoolingExecutor(vertices_batch, edges_batch).pool()  # third pooling\n",
    "\n",
    "        # Fourth conv block\n",
    "        vertices_batch = self.gc4.convolve(vertices_batch, edges_batch)\n",
    "        vertices_batch = self.ln4(vertices_batch)\n",
    "        vertices_batch = nn.functional.leaky_relu(vertices_batch, 0.2)\n",
    "        vertices_batch = self.dropout(vertices_batch)\n",
    "        vertices_batch, edges_batch = PoolingExecutor(vertices_batch, edges_batch).pool()  # fourth pooling\n",
    "\n",
    "        # Fifth conv block\n",
    "        vertices_batch = self.gc5.convolve(vertices_batch, edges_batch)\n",
    "        vertices_batch = self.ln5(vertices_batch)\n",
    "        vertices_batch = nn.functional.leaky_relu(vertices_batch, 0.2)\n",
    "        vertices_batch = self.dropout(vertices_batch)\n",
    "        vertices_batch, edges_batch = PoolingExecutor(vertices_batch, edges_batch).pool()  # fifth pooling\n",
    "\n",
    "        vertices_batch = torch.mean(vertices_batch, dim=1)  # Less than five vertices left, taking their mean just in case there are some orphan vertices. You can continue pooling with RelEdgePool.\n",
    "        vertices_batch = nn.functional.leaky_relu(self.l1(vertices_batch), 0.2)\n",
    "        vertices_batch = self.dropout(vertices_batch)\n",
    "        vertices_batch = self.l2(vertices_batch)\n",
    "\n",
    "        return vertices_batch"
   ],
   "id": "ab5d14e44dbb5bad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Training\n",
    "---"
   ],
   "id": "b51b0969df3fd4ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Training continues as usual.\n",
    "gcnn = DummyGCNN()\n",
    "gcnn.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(gcnn.parameters(), lr=0.1)\n",
    "\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "patience_limit = 100\n",
    "epochs = 400\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for vertices_batch, edges_batch, train_labels in train_loader:\n",
    "        gcnn.train()\n",
    "        optimizer.zero_grad()\n",
    "        preds = gcnn.forward(vertices_batch.clone(), edges_batch.copy())\n",
    "        loss = loss_fn(preds, train_labels)\n",
    "        print(f\"Epoch {epoch}: loss={loss.item():.4f}\")\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(gcnn.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience_limit:\n",
    "            print(f\"Early stopping triggered after {epoch} epochs\")\n",
    "            break"
   ],
   "id": "aecfcaec00590807",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
